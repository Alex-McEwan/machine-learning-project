{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Notebook 6: Reinfocement Learning\n",
    "## Ex. 6: Control problem MountainCar-v0\n",
    "\n",
    "In the exercise class we will cover the control problem of a car at the bottom of a valley which should pick-up enough momentum to get over the hill. We will use the environment from the OpenAI Gym, which allows you to play and visualize the 'game'. Use RL to train a policy that gets the car over the hill in the least amount of time. \n",
    "\n",
    "**Before you can start this exercise you have to install the package OpenAI Gym. Start your anaconda environment with python3 and install:**\n",
    "\n",
    "* pip install gymnasium[classic-control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\alexm\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (4.6.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[classic-control]\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have properly installed the openAI gym you should be able to import it. We will now run a DEMO to see if everything is working. The code already is able to simulate the MountainCar problem for the case that it actions are **random**. To be able to view the rendered video of the poor and helpless car, desperately trying to drive up the hill, you should run the code on your own computer.\n",
    "For more info on this particular environment see e.g. the website: https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    \"\"\"run the MountainCar environment with random actions\"\"\"\n",
    "    \n",
    "    env = gym.make('MountainCar-v0', render_mode='human')  #  create an instance of the environment\n",
    "\n",
    "    state = env.reset()  # reset the current game\n",
    "\n",
    "    for _ in range(200):  # play 200 random actions\n",
    "        env.render()  # render the current game state to screen\n",
    "        a = env.action_space.sample()  # get a random action\n",
    "        state, reward, terminated, truncated, info = env.step(a) # take the action and return the outcome\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "# run the demo \n",
    "demo()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your RL player, ie. training your policy.\n",
    "We have to start by creating the game environment and checking some properties of the state and action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start state: (array([-0.5961045,  0.       ], dtype=float32), {})\n",
      "Number of sctions in the action space: 3\n",
      "Lowest state in the state space: [-1.2  -0.07]\n",
      "Hightest state in the state space: [0.6  0.07]\n",
      "After the step with (a=1): [-5.9556496e-01  5.3951476e-04] -1.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')  # no rendering!\n",
    "\n",
    "# get usefull information about the environment:\n",
    "state = env.reset()\n",
    "print('start state:', state)\n",
    "print('Number of sctions in the action space:', env.action_space.n)\n",
    "print('Lowest state in the state space:', env.observation_space.low)\n",
    "print('Hightest state in the state space:', env.observation_space.high)\n",
    "#perform one step of the game for action a=1\n",
    "a=1\n",
    "state, reward, terminated, truncated, info = env.step(a)\n",
    "print('After the step with (a=1):',state, reward, terminated, truncated, info )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the car starts out in state with two floats [-0.525, 0] (as it initializes random these numbers will differ each time you reset). You can perform any of 3 actions (a = 0 or 1 or 2). We don't know what the numbers in the state mean, they could be the $x$, $y$ coordinates of the car or the velocity and height, but **we also don't have to know!** We will let the RL algortithm learn how to drive the car regardless of the exact meaning of the state.\n",
    "\n",
    "You should now code a function `s2q(s)` that links state `s` to a location in the Q-matrix. This can quickly be done by discretizing the state space into bins and determine the bin number corresponding to a certain value. The function should return a tuple (or list) `loc` that holds the two bin numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soo -0.51498896\n",
      "function test [8, 10]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\AlexM\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction test\u001b[39m\u001b[38;5;124m\"\u001b[39m, [np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], position_bins) , np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], velocity_bins)])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m], position_bins) , np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m1\u001b[39m], velocity_bins)]\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(s2q(state))\n",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m, in \u001b[0;36ms2q\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoo\u001b[39m\u001b[38;5;124m\"\u001b[39m, s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction test\u001b[39m\u001b[38;5;124m\"\u001b[39m, [np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], position_bins) , np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], velocity_bins)])\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m], position_bins) , np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m1\u001b[39m], velocity_bins)]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\AlexM\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:5614\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins, x, side\u001b[38;5;241m=\u001b[39mside)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\AlexM\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1413\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \n\u001b[0;32m   1412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearchsorted\u001b[39m\u001b[38;5;124m'\u001b[39m, v, side\u001b[38;5;241m=\u001b[39mside, sorter\u001b[38;5;241m=\u001b[39msorter)\n",
      "File \u001b[1;32mc:\\Users\\AlexM\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\AlexM\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "def s2q(s):\n",
    "    # convert continous state values to discrete location indices inside the Q matrix\n",
    "    \n",
    "    #----------ADD CODE HERE---------#\n",
    "    position_bins = np.linspace(-1.2, 0.6, 20)\n",
    "    velocity_bins  = np.linspace(-0.07, 0.07, 20)\n",
    "    print(\"soo\", s[0][0])\n",
    "    print(\"function test\", [np.digitize(s[0][0], position_bins) , np.digitize(s[0][1], velocity_bins)])\n",
    "    return [np.digitize(s[0], position_bins) , np.digitize(s[1], velocity_bins)]\n",
    "print(s2q(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function `qlearn()` should train your Q-matrix by playing `num_games` games according to an 'epsilon-greedy' strategy (Google it!) and update the Q-matrix accoding to the following Bellmann equation:\n",
    "\n",
    "$$ \\mathbf{Q}^{\\rm new}[s_t,a_t]=(1-\\alpha)\\mathbf{Q}[s_t,a_t]+\\alpha\\left(R_t+\\gamma\\, \\text{max}_a  \\mathbf{Q}[s_{t+1},a]\\right). $$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate and $\\gamma$ is the discount factor and are bounded by $\\alpha,\\gamma\\in[0,1]$. These parameters have to be set with care, as they influence the speed of convergence of the Q-matrix. The discount factor lets you weigh the importance of future over immediate rewards. This is done by mixing-in the term $\\text{max}_a  \\mathbf{Q}[s_{t+1},a]$, which gives the maximum Q value in the future state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearn(Q, α, γ, ϵ, ϵ_min, num_games):\n",
    "    \"\"\" \n",
    "    learns the Q table by interacting with the environment and applying the Bellman eqation \n",
    "    Q: q-table (n-dimensional ndarray)\n",
    "    α: learning rate\n",
    "    γ: discount factor\n",
    "    ϵ: probability of taking a random action in the epsilon-greedy policy\n",
    "    ϵ_min: minimum value ϵ can take when applying a reduction algortihm to ϵ\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        state = env.reset()  \n",
    "        state = env.step(env.action_space.sample())\n",
    "        state  = s2q(state)\n",
    "        state_pos = state[0]\n",
    "        state_vel = state[1]\n",
    "        print(state)\n",
    "        terminated = False\n",
    "        while terminated == False: \n",
    "            random_number = np.random.uniform(0,1)\n",
    "            if random_number < ϵ:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Q[state_pos][state_vel])\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(a)\n",
    "            next_state = s2q(next_state)\n",
    "            next_state_pos = next_state[0]\n",
    "            next_state_vel = next_state[1]\n",
    "            Q[state_pos][state_vel][a] = Q[state_pos][state_vel][a] + α * (reward + γ * np.max(Q[next_state_pos][next_state_vel][a])) - Q[state_pos][state_vel][a]*α\n",
    "            state = next_state\n",
    "        if ϵ > ϵ_min:\n",
    "            ϵ *= 0.99\n",
    "        wins += 1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #----------ADD CODE HERE---------#\n",
    "\n",
    "    print(f'Training ended. Number of wins: {wins}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you put everything together. It is almost completly finished for you. What values for the hyperparameters do you choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soo -0.5138842\n",
      "function test [8, 10]\n",
      "[array([ 8, 13], dtype=int64), 0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m num_games \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# train the agent and store results\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m qLearn(Q, α, γ, ϵ, ϵ_min, num_games)\n\u001b[0;32m     13\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqrun1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, Q)\n",
      "Cell \u001b[1;32mIn[23], line 28\u001b[0m, in \u001b[0;36mqLearn\u001b[1;34m(Q, α, γ, ε, ε_min, num_games)\u001b[0m\n\u001b[0;32m     25\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[state_pos][state_vel])\n\u001b[0;32m     27\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m---> 28\u001b[0m next_state \u001b[38;5;241m=\u001b[39m s2q(next_state)\n\u001b[0;32m     29\u001b[0m next_state_pos \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m next_state_vel \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36ms2q\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      5\u001b[0m position_bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      6\u001b[0m velocity_bins  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.07\u001b[39m, \u001b[38;5;241m0.07\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoo\u001b[39m\u001b[38;5;124m\"\u001b[39m, s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction test\u001b[39m\u001b[38;5;124m\"\u001b[39m, [np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], position_bins) , np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], velocity_bins)])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m0\u001b[39m], position_bins) , np\u001b[38;5;241m.\u001b[39mdigitize(s[\u001b[38;5;241m1\u001b[39m], velocity_bins)]\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# initialize the Q matrix as a numpy array with zeros\n",
    "Qdim = (20, 20, 3)  \n",
    "Q = np.zeros(shape=Qdim)\n",
    "state = env.reset()\n",
    "# set the hyperparameters\n",
    "α =   0.1\n",
    "γ =   0.1\n",
    "ϵ =   0.1\n",
    "ϵ_min =   0.01\n",
    "num_games = 1000\n",
    "# train the agent and store results\n",
    "qLearn(Q, α, γ, ϵ, ϵ_min, num_games)\n",
    "np.save('qrun1.npy', Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a Q-matrix has been trained we can use it as a policy and play a game. Write code that performs actions according to the input Q-matrix to play a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'qrun1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# replay the game using the trained Q matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqrun1.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# create and reset the environment with render mode on\u001b[39;00m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AlexM\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'qrun1.npy'"
     ]
    }
   ],
   "source": [
    "# replay the game using the trained Q matrix\n",
    "Q = np.load('qrun1.npy')\n",
    "\n",
    "# create and reset the environment with render mode on\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "state = env.reset()  \n",
    "    \n",
    "# play a single episode with max. 1000 actions\n",
    "for _ in range(1000):           \n",
    "    env.render()                \n",
    "    loc = s2q(state)\n",
    "    # a = np.argmax(Q[#----------ADD CODE HERE---------#]) \n",
    "    # state, reward, terminated, truncated, info = env.step(a) \n",
    "        \n",
    "    if terminated: \n",
    "        print('Qplay Output:', reward, terminated, truncated, info)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Set up a couple fo experiments to figure oyt the following things:\n",
    "* How do $\\alpha$ and $\\gamma$ effect your learning perfomance?\n",
    "* Are both elements of the state vector equally important and can we reduce the dimensions of the Q-matrix of one (or both) of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
