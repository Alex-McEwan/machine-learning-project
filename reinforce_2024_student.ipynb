{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Notebook 6: Reinfocement Learning\n",
    "## Ex. 6: Control problem MountainCar-v0\n",
    "\n",
    "In the exercise class we will cover the control problem of a car at the bottom of a valley which should pick-up enough momentum to get over the hill. We will use the environment from the OpenAI Gym, which allows you to play and visualize the 'game'. Use RL to train a policy that gets the car over the hill in the least amount of time. \n",
    "\n",
    "**Before you can start this exercise you have to install the package OpenAI Gym. Start your anaconda environment with python3 and install:**\n",
    "\n",
    "* pip install gymnasium[classic-control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\alexm\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (4.6.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\alexm\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[classic-control]\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have properly installed the openAI gym you should be able to import it. We will now run a DEMO to see if everything is working. The code already is able to simulate the MountainCar problem for the case that it actions are **random**. To be able to view the rendered video of the poor and helpless car, desperately trying to drive up the hill, you should run the code on your own computer.\n",
    "For more info on this particular environment see e.g. the website: https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    \"\"\"run the MountainCar environment with random actions\"\"\"\n",
    "    \n",
    "    env = gym.make('MountainCar-v0', render_mode='human')  #  create an instance of the environment\n",
    "\n",
    "    state = env.reset()  # reset the current game\n",
    "\n",
    "    for _ in range(200):  # play 200 random actions\n",
    "        env.render()  # render the current game state to screen\n",
    "        a = env.action_space.sample()  # get a random action\n",
    "        state, reward, terminated, truncated, info = env.step(a) # take the action and return the outcome\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "# run the demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your RL player, ie. training your policy.\n",
    "We have to start by creating the game environment and checking some properties of the state and action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start state: (array([-0.4464805,  0.       ], dtype=float32), {})\n",
      "Number of sctions in the action space: 3\n",
      "Lowest state in the state space: [-1.2  -0.07]\n",
      "Hightest state in the state space: [0.6  0.07]\n",
      "After the step with (a=1): [-0.44705373 -0.00057324] -1.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')  # no rendering!\n",
    "\n",
    "# get usefull information about the environment:\n",
    "state = env.reset()\n",
    "print('start state:', state)\n",
    "print('Number of sctions in the action space:', env.action_space.n)\n",
    "print('Lowest state in the state space:', env.observation_space.low)\n",
    "print('Hightest state in the state space:', env.observation_space.high)\n",
    "#perform one step of the game for action a=1\n",
    "a=1\n",
    "state, reward, terminated, truncated, info = env.step(a)\n",
    "print('After the step with (a=1):',state, reward, terminated, truncated, info )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the car starts out in state with two floats [-0.525, 0] (as it initializes random these numbers will differ each time you reset). You can perform any of 3 actions (a = 0 or 1 or 2). We don't know what the numbers in the state mean, they could be the $x$, $y$ coordinates of the car or the velocity and height, but **we also don't have to know!** We will let the RL algortithm learn how to drive the car regardless of the exact meaning of the state.\n",
    "\n",
    "You should now code a function `s2q(s)` that links state `s` to a location in the Q-matrix. This can quickly be done by discretizing the state space into bins and determine the bin number corresponding to a certain value. The function should return a tuple (or list) `loc` that holds the two bin numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 5]\n"
     ]
    }
   ],
   "source": [
    "position_bin_count = 20\n",
    "velocity_bin_count = 20\n",
    "def s2q(s):\n",
    "    # convert continous state values to discrete location indices inside the Q matrix\n",
    "    \n",
    "    #----------ADD CODE HERE---------#\n",
    "    position_bins = np.linspace(-1.2, 0.6, position_bin_count)\n",
    "    velocity_bins  = np.linspace(-0.07, 0.07, velocity_bin_count)\n",
    "    return [np.digitize(s[0], position_bins) , np.digitize(s[1], velocity_bins)]\n",
    "print(s2q(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function `qlearn()` should train your Q-matrix by playing `num_games` games according to an 'epsilon-greedy' strategy (Google it!) and update the Q-matrix accoding to the following Bellmann equation:\n",
    "\n",
    "$$ \\mathbf{Q}^{\\rm new}[s_t,a_t]=(1-\\alpha)\\mathbf{Q}[s_t,a_t]+\\alpha\\left(R_t+\\gamma\\, \\text{max}_a  \\mathbf{Q}[s_{t+1},a]\\right). $$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate and $\\gamma$ is the discount factor and are bounded by $\\alpha,\\gamma\\in[0,1]$. These parameters have to be set with care, as they influence the speed of convergence of the Q-matrix. The discount factor lets you weigh the importance of future over immediate rewards. This is done by mixing-in the term $\\text{max}_a  \\mathbf{Q}[s_{t+1},a]$, which gives the maximum Q value in the future state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearn(Q, α, γ, ϵ, ϵ_min, num_games):\n",
    "    \"\"\" \n",
    "    learns the Q table by interacting with the environment and applying the Bellman eqation \n",
    "    Q: q-table (n-dimensional ndarray)\n",
    "    α: learning rate\n",
    "    γ: discount factor\n",
    "    ϵ: probability of taking a random action in the epsilon-greedy policy\n",
    "    ϵ_min: minimum value ϵ can take when applying a reduction algortihm to ϵ\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "\n",
    "    for i in range(num_games):\n",
    "        state = env.reset()[0]  \n",
    "        state  = s2q(state)\n",
    "        state_pos = state[0]\n",
    "        state_vel = state[1]\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        for j in range(1000): \n",
    "            if terminated: \n",
    "                break\n",
    "            random_number = np.random.uniform(0,1)\n",
    "            if random_number < ϵ:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Q[state_pos][state_vel])\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(a)\n",
    "            next_state = s2q(next_state)\n",
    "            next_state_pos = next_state[0]\n",
    "            next_state_vel = next_state[1]\n",
    "            if not terminated:\n",
    "                Q[state_pos][state_vel][a] = Q[state_pos][state_vel][a] + α * (reward + γ * np.max(Q[next_state_pos][next_state_vel]) - Q[state_pos][state_vel][a])\n",
    "            else:\n",
    "                Q[state_pos][state_vel][a] = Q[state_pos][state_vel][a] +  α*(reward - Q[state_pos][state_vel][a])\n",
    "                wins += 1\n",
    "                break\n",
    "            state_pos = next_state_pos\n",
    "            state_vel = next_state_vel\n",
    "        if ϵ > ϵ_min:\n",
    "            ϵ *= 0.99\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #----------ADD CODE HERE---------#\n",
    "\n",
    "    print(f'Training ended. Number of wins: {wins}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you put everything together. It is almost completly finished for you. What values for the hyperparameters do you choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended. Number of wins: 12\n"
     ]
    }
   ],
   "source": [
    "# initialize the Q matrix as a numpy array with zeros\n",
    "Qdim = (position_bin_count, velocity_bin_count, 3)  \n",
    "Q = np.zeros(shape=Qdim)\n",
    "state = env.reset()\n",
    "# set the hyperparameters\n",
    "α =   0.3\n",
    "γ =   0.9\n",
    "ϵ =   1\n",
    "ϵ_min =   0.1\n",
    "num_games = 100\n",
    "# train the agent and store results\n",
    "qLearn(Q, α, γ, ϵ, ϵ_min, num_games)\n",
    "np.save('qrun1.npy', Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a Q-matrix has been trained we can use it as a policy and play a game. Write code that performs actions according to the input Q-matrix to play a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay the game using the trained Q matrix\n",
    "Q = np.load('qrun1.npy')\n",
    "\n",
    "# create and reset the environment with render mode on\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "state = env.reset()[0]\n",
    "    \n",
    "# play a single episode with max. 1000 actions\n",
    "for _ in range(1000):           \n",
    "    env.render()                \n",
    "    loc = s2q(state)\n",
    "    state_pos = loc[0]\n",
    "    state_vel = loc[1]\n",
    "    a = np.argmax(Q[state_pos][state_vel])\n",
    "    state, reward, terminated, truncated, info = env.step(a) \n",
    "        \n",
    "    if terminated: \n",
    "        print('Qplay Output:', reward, terminated, truncated, info)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Set up a couple fo experiments to figure oyt the following things:\n",
    "* How do $\\alpha$ and $\\gamma$ effect your learning perfomance?\n",
    "* Are both elements of the state vector equally important and can we reduce the dimensions of the Q-matrix of one (or both) of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the discount factor should be quite high. If its too low the algorithm isnt able to form a proper strategy based on future rewards and gets very little wins. When i decreased it from 0.9 to 0.5 the ammount of wins halved. Also the learning rate should be somewhere in between 0.1 and 0.5 With 0.1 it will converge but it takes a bit longer and with 0.6 it probably over shoots since the number of wins decreases again. So now i set it to 0.3. As for wich element of the state vector is more important, its position. When i decrease the ammount of bins in the position array it has a more significant impact than when i do this for the velocity array, in terms of the ammount of wins obtained in training. This indicates that position holds a higher significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
